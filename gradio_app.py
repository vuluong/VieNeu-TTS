import os
import re
import time
import hashlib
import gradio as gr
import soundfile as sf
import numpy as np
from utils.logging import setup_logging, get_logger
from utils.normalize_text import VietnameseTTSNormalizer

# === Logging ===
setup_logging(run_name="vieneu-gradio", to_file=True, log_dir="logs", level="INFO")
log = get_logger("app.gradio")

log.info("Kh·ªüi ƒë·ªông VieNeu-TTS (Gradio) ...")

from vieneu_tts.vieneu_tts import VieNeuTTS

# --- c·∫•u h√¨nh th∆∞ m·ª•c output ---
OUTPUT_DIR = "output_audio"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- c·∫•u h√¨nh th∆∞ m·ª•c cache ref_codes ---
REF_CACHE_DIR = "cache_ref"
os.makedirs(REF_CACHE_DIR, exist_ok=True)
_REF_MEM: dict[str, np.ndarray] = {}  # cache trong RAM

# --- normalizer ---
TEXT_NORMALIZER = VietnameseTTSNormalizer()

def _slug(s: str, maxlen: int = 40) -> str:
    s = re.sub(r"[^a-zA-Z0-9._-]+", "_", s).strip("_")
    return s[:maxlen] if len(s) > maxlen else s

def _make_outpath(text: str, voice_choice: str | None) -> str:
    ts = time.strftime("%Y%m%d_%H%M%S")
    h8 = hashlib.sha1((text or "").encode("utf-8")).hexdigest()[:8]
    vslug = _slug(voice_choice or "custom")
    fname = f"{ts}_{vslug}_{h8}.wav"
    return os.path.join(OUTPUT_DIR, fname)

def _hash_file(path: str) -> str:
    try:
        st = os.stat(path)
        key = f"{os.path.abspath(path)}|{st.st_size}|{int(st.st_mtime)}"
        return hashlib.md5(key.encode("utf-8")).hexdigest()
    except Exception:
        return hashlib.md5((path or "none").encode("utf-8")).hexdigest()

def _cache_file_for_key(cache_key: str) -> str:
    # l∆∞u tr√™n ƒëƒ©a theo md5 c·ªßa cache_key, tr√°nh k√Ω t·ª± l·∫°
    return os.path.join(REF_CACHE_DIR, hashlib.md5(cache_key.encode("utf-8")).hexdigest() + ".npy")

def _cache_get(cache_key: str) -> tuple[np.ndarray | None, bool]:
    # 1) RAM
    arr = _REF_MEM.get(cache_key)
    if arr is not None:
        return arr, True
    # 2) ƒêƒ©a
    fpath = _cache_file_for_key(cache_key)
    if os.path.exists(fpath):
        try:
            arr = np.load(fpath)
            _REF_MEM[cache_key] = arr
            return arr, True
        except Exception as e:
            log.warning("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c cache ƒëƒ©a %s: %s", fpath, e)
    return None, False

def _cache_put(cache_key: str, arr: np.ndarray) -> None:
    _REF_MEM[cache_key] = arr
    fpath = _cache_file_for_key(cache_key)
    try:
        # √©p int32 cho an to√†n
        np.save(fpath, arr.astype(np.int32, copy=False))
    except Exception as e:
        log.warning("Kh√¥ng ghi ƒë∆∞·ª£c cache ƒëƒ©a %s: %s", fpath, e)

tts = VieNeuTTS(
    backbone_repo="pnnbao-ump/VieNeu-TTS",
    codec_repo="neuphonic/neucodec",
)
log.info("Model ƒë√£ t·∫£i xong")

# === Danh s√°ch gi·ªçng m·∫´u ===
VOICE_SAMPLES = {
    "Nam 1 (id_0001)": {"audio": "./sample/id_0001.wav", "text": "./sample/id_0001.txt"},
    "N·ªØ 1 (id_0002)": {"audio": "./sample/id_0002.wav", "text": "./sample/id_0002.txt"},
    "Nam 2 (id_0003)": {"audio": "./sample/id_0003.wav", "text": "./sample/id_0003.txt"},
    "N·ªØ 2 (id_0004)": {"audio": "./sample/id_0004.wav", "text": "./sample/id_0004.txt"},
    "Nam 3 (id_0005)": {"audio": "./sample/id_0005.wav", "text": "./sample/id_0005.txt"},
    "Nam 4 (id_0007)": {"audio": "./sample/id_0007.wav", "text": "./sample/id_0007.txt"},
}

def _resolve_ref(voice_choice: str, custom_audio: str | None, custom_text: str | None):
    """
    Tr·∫£ v·ªÅ (ref_audio_path, ref_text, cache_key, src_desc)
    cache_key: x√°c ƒë·ªãnh duy nh·∫•t theo preset/custom + hash file
    """
    if custom_audio and custom_text:
        audio_path = custom_audio
        ref_text = custom_text
        cache_key = f"custom:{_hash_file(audio_path)}"
        src_desc = "gi·ªçng t√πy ch·ªânh"
        return audio_path, ref_text, cache_key, src_desc

    if voice_choice in VOICE_SAMPLES:
        audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        with open(text_path, "r", encoding="utf-8") as f:
            ref_text = f.read()
        cache_key = f"preset:{voice_choice}:{_hash_file(audio_path)}"
        src_desc = f"gi·ªçng m·∫´u {voice_choice}"
        return audio_path, ref_text, cache_key, src_desc

    raise ValueError("Vui l√≤ng ch·ªçn gi·ªçng ho·∫∑c t·∫£i audio + text t√πy ch·ªânh.")

def _load_ref(voice_choice: str, custom_audio: str | None, custom_text: str | None):
    """
    Tr·∫£ v·ªÅ (ref_codes(np.ndarray[int32]), ref_text(str), cache_key(str), is_cache_hit(bool))
    """
    ref_audio, ref_text, cache_key, src_desc = _resolve_ref(voice_choice, custom_audio, custom_text)

    # 1) th·ª≠ l·∫•y t·ª´ cache
    arr, hit = _cache_get(cache_key)
    if hit and isinstance(arr, np.ndarray) and arr.ndim == 1:
        log.info("REF CACHE HIT: %s ‚Üí %s tokens", src_desc, arr.shape[0])
        return arr, ref_text, cache_key, True

    # 2) kh√¥ng c√≥ cache ‚Üí encode
    log.info("REF CACHE MISS: %s ‚Üí encode_reference()", src_desc)
    t0 = time.perf_counter()
    ref_codes_tensor = tts.encode_reference(ref_audio)  # c√≥ th·ªÉ l√† torch.Tensor
    dt_ms = (time.perf_counter() - t0) * 1000.0

    # √©p v·ªÅ np.ndarray[int32] ƒë·ªÉ cache, tr√°nh chi·∫øm VRAM
    if hasattr(ref_codes_tensor, "detach"):
        ref_codes_np = ref_codes_tensor.detach().cpu().numpy().astype(np.int32, copy=False)
    elif isinstance(ref_codes_tensor, np.ndarray):
        ref_codes_np = ref_codes_tensor.astype(np.int32, copy=False)
    else:
        # list-like
        ref_codes_np = np.asarray(list(ref_codes_tensor), dtype=np.int32)

    log.info("encode_reference() xong: %.1f ms, codes=%s", dt_ms, tuple(ref_codes_np.shape))
    _cache_put(cache_key, ref_codes_np)
    return ref_codes_np, ref_text, cache_key, False

# --- preview normalize (hi·ªÉn th·ªã sau chu·∫©n ho√°) ---
def preview_normalize(text: str, use_normalizer: bool) -> str:
    if use_normalizer and text:
        try:
            return TEXT_NORMALIZER.normalize(text)
        except Exception as e:
            log.exception("L·ªói normalize preview: %s", e)
            return ""
    return ""

def synthesize_speech(text, voice_choice, custom_audio=None, custom_text=None, use_normalizer: bool = False):
    try:
        if not text or text.strip() == "":
            return None, "‚ùå Vui l√≤ng nh·∫≠p vƒÉn b·∫£n c·∫ßn t·ªïng h·ª£p", ""

        if len(text) > 250:
            return None, "‚ùå VƒÉn b·∫£n qu√° d√†i! Vui l√≤ng nh·∫≠p t·ªëi ƒëa 250 k√Ω t·ª±", ""

        # n·∫øu b·∫≠t normalizer -> d√πng text ƒë√£ chu·∫©n ho√°
        text_to_use = TEXT_NORMALIZER.normalize(text) if use_normalizer else text
        norm_preview = text_to_use if use_normalizer else ""

        ref_codes, ref_text, cache_key, hit = _load_ref(voice_choice, custom_audio, custom_text)

        log.info("Infer: len(text)=%d, cache_hit=%s, normalized=%s",
                 len(text_to_use), hit, use_normalizer)
        t0 = time.perf_counter()
        wav = tts.infer(text_to_use, ref_codes, ref_text)
        dt = (time.perf_counter() - t0) * 1000
        log.info("infer() xong: %.1f ms", dt)

        out_path = _make_outpath(text_to_use, voice_choice)
        sf.write(out_path, wav, tts.sample_rate)
        log.info("L∆∞u file: %s", out_path)

        cache_str = "cache=hit" if hit else "cache=miss"
        norm_str = "normalize=on" if use_normalizer else "normalize=off"
        return out_path, f"‚úÖ T·ªïng h·ª£p th√†nh c√¥ng! {cache_str} | {norm_str} | time: {dt:.1f} ms", norm_preview
    except Exception as e:
        log.exception("L·ªói synthesize_speech: %s", e)
        return None, f"‚ùå L·ªói: {str(e)}", ""

examples = [
    ["Legacy l√† m·ªôt b·ªô phim ƒë·ªôt ph√° v·ªÅ m·∫∑t √¢m nh·∫°c, quay phim, hi·ªáu ·ª©ng ƒë·∫∑c bi·ªát, v√† t√¥i r·∫•t m·ª´ng v√¨ cu·ªëi c√πng n√≥ c≈©ng ƒë∆∞·ª£c c·∫£ gi·ªõi ph√™ b√¨nh l·∫´n ng∆∞·ªùi h√¢m m·ªô ƒë√°nh gi√° l·∫°i. Ch√∫ng ta ƒë√£ qu√° b·∫•t c√¥ng v·ªõi b·ªô phim n√†y v√†o nƒÉm 2010.", "Nam 1 (id_0001)"],
    ["T·ª´ nhi·ªÅu ngu·ªìn t√†i li·ªáu l·ªãch s·ª≠, c√≥ th·ªÉ th·∫•y nu√¥i con theo phong c√°ch Do Th√°i kh√¥ng ch·ªâ t·ªët cho ƒë·ª©a tr·∫ª m√† c√≤n t·ªët cho c·∫£ c√°c b·∫≠c cha m·∫π.", "N·ªØ 1 (id_0002)"],
    ["C√°c b√°c sƒ© ƒëang nghi√™n c·ª©u m·ªôt lo·∫°i vaccine m·ªõi ch·ªëng l·∫°i virus c√∫m m√πa. Th√≠ nghi·ªám l√¢m s√†ng cho th·∫•y ph·∫£n ·ª©ng mi·ªÖn d·ªãch m·∫°nh m·∫Ω v√† √≠t t√°c d·ª•ng ph·ª•, m·ªü ra hy v·ªçng ph√≤ng ch·ªëng d·ªãch b·ªánh hi·ªáu qu·∫£ h∆°n trong t∆∞∆°ng lai.", "Nam 2 (id_0003)"],
]

custom_css = """
.gradio-container { max-width: 1000px !important; margin: 0 auto !important; padding: 20px !important; }
.contain { max-width: 1000px !important; margin: 0 auto !important; }
#warning { background-color: #fff3cd; border: 1px solid #ffc107; border-radius: 5px; padding: 10px; margin: 10px 0; }
#info { background-color: #d1ecf1; border: 1px solid #17a2b8; border-radius: 5px; padding: 10px; margin: 10px 0; }
"""

with gr.Blocks(title="VieNeu-TTS Local", css=custom_css, theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # üéôÔ∏è VieNeu-TTS: Vietnamese Text-to-Speech (Local Version)

    H·ªá th·ªëng t·ªïng h·ª£p ti·∫øng n√≥i ti·∫øng Vi·ªát ƒë∆∞·ª£c **finetune t·ª´ NeuTTS-Air**.

    T√°c gi·∫£: [Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o](https://github.com/pnnbao97)  
    Model: [VieNeu-TTS](https://huggingface.co/pnnbao-ump/VieNeu-TTS)  
    Code: [GitHub](https://github.com/pnnbao97/VieNeu-TTS)
    """)
    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(
                label="üìù VƒÉn b·∫£n ƒë·∫ßu v√†o (t·ªëi ƒëa 250 k√Ω t·ª±)",
                placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Vi·ªát...",
                lines=4,
                max_lines=6,
                value="Legacy l√† m·ªôt b·ªô phim ƒë·ªôt ph√° v·ªÅ m·∫∑t √¢m nh·∫°c, quay phim, hi·ªáu ·ª©ng ƒë·∫∑c bi·ªát, v√† t√¥i r·∫•t m·ª´ng v√¨ cu·ªëi c√πng n√≥ c≈©ng ƒë∆∞·ª£c c·∫£ gi·ªõi ph√™ b√¨nh l·∫´n ng∆∞·ªùi h√¢m m·ªô ƒë√°nh gi√° l·∫°i. Ch√∫ng ta ƒë√£ qu√° b·∫•t c√¥ng v·ªõi b·ªô phim n√†y v√†o nƒÉm 2010."
            )
            char_count = gr.Markdown("209 / 250 k√Ω t·ª±")

            voice_select = gr.Radio(
                choices=list(VOICE_SAMPLES.keys()),
                label="üé§ Ch·ªçn gi·ªçng m·∫´u",
                value="Nam 1 (id_0001)",
                info="Gi·ªçng l·∫ª: Nam | Gi·ªçng ch·∫µn: N·ªØ"
            )

            with gr.Accordion("üé® Ho·∫∑c s·ª≠ d·ª•ng gi·ªçng t√πy ch·ªânh", open=False):
                gr.Markdown("""
                **H∆∞·ªõng d·∫´n:**
                - Upload file audio (.wav) v√† nh·∫≠p n·ªôi dung text ch√≠nh x√°c t∆∞∆°ng ·ª©ng
                - **L∆∞u √Ω:** Ch·∫•t l∆∞·ª£ng c√≥ th·ªÉ kh√¥ng t·ªët b·∫±ng c√°c gi·ªçng m·∫´u
                """)
                custom_audio = gr.Audio(label="File audio m·∫´u", type="filepath")
                custom_text = gr.Textbox(label="N·ªôi dung c·ªßa audio m·∫´u", placeholder="Nh·∫≠p ch√≠nh x√°c n·ªôi dung...", lines=2)

            submit_btn = gr.Button("üéµ T·ªïng h·ª£p gi·ªçng n√≥i", variant="primary", size="lg")

            # Checkbox ƒë∆∞·ª£c ƒë·∫∑t D∆Ø·ªöI n√∫t
            use_normalizer = gr.Checkbox(label="üßπ B·∫≠t chu·∫©n ho√° vƒÉn b·∫£n", value=False)

        with gr.Column():
            # Audio t·ª± ph√°t
            audio_output = gr.Audio(label="üîä K·∫øt qu·∫£", autoplay=True)
            # "Sau chu·∫©n ho√°" chuy·ªÉn sang c·ªôt ph·∫£i, ƒë·∫∑t TR√äN kh·ªëi tr·∫°ng th√°i
            norm_output = gr.Textbox(label="üîß Sau chu·∫©n ho√° (ch·ªâ hi·ªán khi b·∫≠t)", interactive=False, lines=4)
            status_output = gr.Textbox(label="üìä Tr·∫°ng th√°i", interactive=False)

    gr.Markdown("### üí° V√≠ d·ª• nhanh")
    gr.Examples(
        examples=examples,
        inputs=[text_input, voice_select],
        outputs=[audio_output, status_output, norm_output],
        fn=synthesize_speech,
        cache_examples=False
    )

    def update_char_count(text):
        count = len(text) if text else 0
        color = "red" if count > 250 else "green"
        return f"<span style='color: {color}'>{count} / 250 k√Ω t·ª±</span>"

    # c·∫≠p nh·∫≠t preview normalize theo text + checkbox
    text_input.change(
        fn=preview_normalize,
        inputs=[text_input, use_normalizer],
        outputs=[norm_output],
    )
    use_normalizer.change(
        fn=preview_normalize,
        inputs=[text_input, use_normalizer],
        outputs=[norm_output],
    )

    text_input.change(fn=update_char_count, inputs=[text_input], outputs=[char_count])

    submit_btn.click(
        fn=synthesize_speech,
        inputs=[text_input, voice_select, custom_audio, custom_text, use_normalizer],
        outputs=[audio_output, status_output, norm_output],
    )

if __name__ == "__main__":
    demo.queue(max_size=20)
    demo.launch(share=False, server_name="127.0.0.1", server_port=7860, show_error=True)
